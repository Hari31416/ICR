{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    classification_report,\n    ConfusionMatrixDisplay,\n    roc_auc_score,\n)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport optuna\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:00:24.037298Z","iopub.status.idle":"2023-06-17T11:00:24.037777Z","shell.execute_reply.started":"2023-06-17T11:00:24.037539Z","shell.execute_reply":"2023-06-17T11:00:24.037562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:00:24.039589Z","iopub.status.idle":"2023-06-17T11:00:24.040081Z","shell.execute_reply.started":"2023-06-17T11:00:24.039818Z","shell.execute_reply":"2023-06-17T11:00:24.039840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"kaggle = \"KAGGLE\" in \"\".join(os.environ.keys())","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:00:24.043668Z","iopub.status.idle":"2023-06-17T11:00:24.044302Z","shell.execute_reply.started":"2023-06-17T11:00:24.044050Z","shell.execute_reply":"2023-06-17T11:00:24.044073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    train = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/train.csv\")\n    test = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\")\n    greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\nelse:\n    train = pd.read_csv(\"data/train.csv\")\n    test = pd.read_csv(\"data/test.csv\")\n    greeks = pd.read_csv(\"data/greeks.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"markdown","source":"## Preliminaries","metadata":{}},{"cell_type":"markdown","source":"Label encode the `EJ` column","metadata":{}},{"cell_type":"code","source":"train[\"EJ\"].replace({\"A\": 0, \"B\": 1}, inplace=True)\ntest[\"EJ\"].replace({\"A\": 0, \"B\": 1}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop extra columns:","metadata":{}},{"cell_type":"code","source":"train_id = train[\"Id\"]\ntest_id = test[\"Id\"]\ny = train[\"Class\"]\n\ntrain = train.drop([\"Class\"], axis=1)\ntrain = train.drop([\"Id\"], axis=1)\ntest  = test.drop([\"Id\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the columns for imputation:","metadata":{}},{"cell_type":"code","source":"train_null_columns = list(train.columns[train.isna().sum() != 0])\ntest_null_columns = list(test.columns[test.isna().sum() != 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = 2\ntrain_columns_to_fill_via_mean = []\ntrain_columns_to_fill_via_knn = []\n\nnull_count = train[train_null_columns].isna().sum()\nfor column in train_null_columns:\n    if null_count[column] <= thresh:\n        train_columns_to_fill_via_mean.append(column)\n    else:\n        train_columns_to_fill_via_knn.append(column)\n\ntrain_columns_to_fill_via_mean, train_columns_to_fill_via_knn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = 2\ntest_columns_to_fill_via_mean = []\ntest_columns_to_fill_via_knn = []\n\nnull_count = test[test_null_columns].isna().sum()\nfor column in test_null_columns:\n    if null_count[column] <= thresh:\n        test_columns_to_fill_via_mean.append(column)\n    else:\n        test_columns_to_fill_via_knn.append(column)\n\ntest_columns_to_fill_via_mean, test_columns_to_fill_via_knn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipelines","metadata":{}},{"cell_type":"markdown","source":"Now, the column transformations:","metadata":{}},{"cell_type":"code","source":"simple_imputer = SimpleImputer(strategy=\"mean\")\nknn_imputer = KNNImputer(n_neighbors=5)\n\ntrain_imputer = ColumnTransformer(\n    [\n        (\"mean_imputer\", simple_imputer, train_columns_to_fill_via_mean),\n        (\"knn_imputer\", knn_imputer, train_columns_to_fill_via_knn),\n    ],\n    remainder=\"passthrough\",\n)\n\ntest_imputer = ColumnTransformer(\n    [\n        (\"mean_imputer\", simple_imputer, test_columns_to_fill_via_mean),\n        (\"knn_imputer\", knn_imputer, test_columns_to_fill_via_knn),\n    ],\n    remainder=\"passthrough\",\n)\n\nstandard_scaler = StandardScaler()\npower_transformer = PowerTransformer()\n\ntrain_scaling_pipe = Pipeline(\n    [\n        (\"standard_scaler\", standard_scaler),\n        (\"power_transformer\", power_transformer),\n    ],\n)\n\ntest_scaling_pipe = Pipeline(\n    [\n        (\"standard_scaler\", standard_scaler),\n        (\"power_transformer\", power_transformer),\n    ],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the final data preprocessor:","metadata":{}},{"cell_type":"code","source":"train_final_preprocessing_pipe = Pipeline([\n    (\"imputer\", train_imputer),\n    (\"scaling_pipe\", train_scaling_pipe)\n])\n\ntest_final_preprocessing_pipe = Pipeline([\n    (\"imputer\", test_imputer),\n    (\"scaling_pipe\", test_scaling_pipe)\n])\n\ntrain_final_preprocessing_pipe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Now, we will create the dataset to be trained:","metadata":{}},{"cell_type":"code","source":"X = train_final_preprocessing_pipe.fit_transform(train)\ntest_final = test_final_preprocessing_pipe.fit_transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isnan(X).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will make sure that both the training and test sets have the same columns:","metadata":{}},{"cell_type":"code","source":"assert X.shape[1] == test_final.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"markdown","source":"Great Let's do a train test split:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions  ","metadata":{}},{"cell_type":"markdown","source":"We will create some helper functions to help us train and evaluate our models:","metadata":{}},{"cell_type":"code","source":"def balanced_log_loss(y_true, y_hat):\n    \"\"\"\n    Compute the balanced log loss between y_true and y_hat.\n\n    Parameters:\n    y_true (array-like): True labels of shape (n_samples, n_classes).\n    y_hat (array-like): Predicted probabilities of shape (n_samples, n_classes).\n\n    Returns:\n    float: The balanced log loss between y_true and y_hat.\n    \"\"\"\n    eps = 1e-15\n    y_hat = np.clip(y_hat, eps, 1 - eps)\n    if isinstance(y_true, pd.Series):\n        y_true = y_true.values\n    if isinstance(y_hat, pd.Series):\n        y_hat = y_hat.values\n    \n    y_true = y_true.astype(int)\n    N0 = np.sum(y_true == 0)\n    N1 = np.sum(y_true == 1)\n    w0 = 1/N0\n    w1 = 1/N1\n    yhat0 = y_hat[:, 0]\n    yhat1 = y_hat[:, 1]\n    loss0 = -w0 * np.sum((1 -y_true) * np.log(yhat0))\n    loss1 = -w1 * np.sum((y_true) * np.log(yhat1))\n    return (loss0 + loss1)/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def balanced_log_loss(y_true, y_pred):\n#     # calculate the number of observations for each class\n#     N_0 = np.sum(1 - y_true)\n#     N_1 = np.sum(y_true)\n#     # calculate the weights for each class\n#     w_0 = 1 / N_0\n#     w_1 = 1 / N_1\n#     # calculate the predicted probabilities for each class\n#     p_0 = np.clip(y_pred[:, 0], 1e-15, 1 - 1e-15)\n#     p_1 = np.clip(y_pred[:, 1], 1e-15, 1 - 1e-15)\n#     # calculate the log loss for each class\n#     log_loss_0 = -w_0 * np.sum((y_true) * np.log(p_0))\n#     log_loss_1 = -w_1 * np.sum(y_true * np.log(p_1))\n#     # calculate the balanced logarithmic loss\n#     balanced_log_loss = (log_loss_0 + log_loss_1) / (w_0 + w_1)\n#     return balanced_log_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cm_to_metrics(cm):\n    \"\"\"Calculate accuracy, precision, recall and f1 score from confusion matrix.\n\n    Parameters\n    ----------\n    cm : array-like\n        Confusion matrix.\n\n    Returns\n    -------\n    accuracy : float\n        Accuracy score.\n    precision : float\n        Precision score.\n    recall : float\n        Recall score.\n    f1 : float\n        F1 score.\n    \"\"\"\n    tn, fp, fn, tp = cm.ravel()\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1 = 2 * (precision * recall) / (precision + recall)\n    return accuracy, precision, recall, f1\n\n\ndef evaluate_model(\n    model,\n    on=\"train\",\n    plot_cmat=False,\n    verbose=True,\n):\n    \"\"\"\n    This function evaluates a model and returns the metrics.\n    It can be used to evaluate the model on the training set or the test set.\n    It can also plot the confusion matrix.\n    Parameters\n    ----------\n    model : object\n        The model to be evaluated.\n    on : str, optional\n        The set on which the model will be evaluated. The default is \"train\".\n    plot_cmat : bool, optional\n        Whether to plot the confusion matrix. The default is False.\n    verbose : bool, optional\n        Whether to print the metrics. The default is True.\n\n    Returns\n    -------\n    result : dict\n        A dictionary with the metrics.\n\n    Example\n    -------\n    >>> result = evaluate_model(model)\n    >>> print(result)\n    {'accuracy': 0.8, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'auc': 0.8}\n\n    >>> result = evaluate_model(model, on=\"test\")\n    >>> print(result)\n    {'accuracy': 0.8, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'auc': 0.8}\n\n    >>> result = evaluate_model(model, plot_cmat=True)\n    >>> print(result)\n    {'accuracy': 0.8, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'auc': 0.8}\n\n    >>> result = evaluate_model(model, on=\"test\", plot_cmat=True)\n    >>> print(result)\n    {'accuracy': 0.8, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'auc': 0.8}\n    \"\"\"\n    if on == \"train\":\n        X = X_train\n        y = y_train\n    else:\n        X = X_test\n        y = y_test\n    y_pred = model.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    accuracy, precision, recall, f1 = cm_to_metrics(cm)\n    auc_score = roc_auc_score(y, y_pred)\n    y_pred_prob = model.predict_proba(X)\n    balanced_ll = balanced_log_loss(y, y_pred_prob)\n    \n\n    if plot_cmat:\n        disp = ConfusionMatrixDisplay(\n            confusion_matrix=cm, display_labels=[\"0\", \"1\"]\n        )\n        disp.plot()\n        plt.show()\n    if verbose:\n        try:\n            model_name = model.__class__.__name__\n        except:\n            model_name = \"\"\n        print(f\"Accuracy on {on} set of the model {model_name}: {accuracy:.4f}\")\n        print(f\"Log Loss on {on} set of the model {model_name}: {balanced_ll:.4f}\")\n        print(f\"Precision on {on} set of the model {model_name}: {precision:.4f}\")\n        print(f\"Recall on {on} set of the model {model_name}: {recall:.4f}\")\n        print(f\"F1 on {on} set of the model {model_name}: {f1:.4f}\")\n        print(f\"AUC on {on} set of the model {model_name}: {auc_score:.4f}\\n\")\n        cr = classification_report(y, y_pred)\n        print(cr)\n    result = {\n        \"accuracy\": accuracy,\n        \"log_loss\": balanced_ll,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"auc\": auc_score,\n    }\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"one_weight = 1/y.mean()\nzero_weight = 1/(1- y.mean())\nclass_weight = {0:1, 1:one_weight/zero_weight}\nclass_weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model","metadata":{}},{"cell_type":"markdown","source":"Let's train a base model. This will be a simple linear regression model:","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(class_weight=class_weight)\nlr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_result_train = evaluate_model(lr, plot_cmat=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_result_test = evaluate_model(lr, plot_cmat=True, on = \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"rf_base = RandomForestClassifier(class_weight=class_weight, max_depth=5, n_estimators=100)\nrf_base.fit(X_train, y_train)\nevaluate_model(rf_base, plot_cmat=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(rf_base, plot_cmat=True, on = \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RF Tuning Round 1","metadata":{}},{"cell_type":"code","source":"def custom_scorere_func(model, X, y):\n    y_pred = model.predict_proba(X)\n    return balanced_log_loss(y, y_pred)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grid_search(params, base_model, cv = 5, verbose = 1, **kwargs):\n    grid = GridSearchCV(base_model, params, cv = cv, scoring = custom_scorere_func, verbose = verbose, **kwargs)\n    grid.fit(X_train, y_train)\n    print(grid.best_params_)\n    print(grid.best_score_)\n    return grid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rfc_param_grid = {\n#     \"n_estimators\": [100, 200, 300],\n#     \"max_depth\": [5, 10, 15],\n#     \"min_samples_split\": [2, 5, 10],\n#     \"min_samples_leaf\": [1, 2, 5],\n#     \"max_features\": [\"sqrt\", \"log2\"],\n# }\n\n# rfc_grid = grid_search(rfc_param_grid, RandomForestClassifier(random_state=RANDOM_SEED), verbose = 1, n_jobs = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RF Tuning Round 2","metadata":{}},{"cell_type":"code","source":"# rfc_param_grid = {\n#     \"n_estimators\": [170, 200, 230],\n#     \"max_depth\": [4, 5, 7],\n#     \"min_samples_split\": [4, 5, 6],\n#     \"min_samples_leaf\": [1],\n#     \"max_features\": [\"log2\"],\n# }\n\n# rfc_grid = grid_search(rfc_param_grid, RandomForestClassifier(random_state=RANDOM_SEED), verbose = 1, n_jobs = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RF Tuning Round 3","metadata":{}},{"cell_type":"code","source":"# rfc_param_grid = {\n#     \"n_estimators\": [190, 200, 210],\n#     \"max_depth\": [3, 4],\n#     \"min_samples_split\": [6, 7, 8],\n#     \"min_samples_leaf\": [1],\n#     \"max_features\": [\"log2\"],\n# }\n\n# rfc_grid = grid_search(rfc_param_grid, RandomForestClassifier(random_state=RANDOM_SEED), verbose = 1, n_jobs = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the same parameters as we got in round 1 and hence we will not proceed to a round 3 of fine tuning. Here are the best parameters:\n```python\n{'max_depth': 17, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 190}\n```","metadata":{}},{"cell_type":"code","source":"# # rf_best_params = rfc_grid.best_params_\n# # rf_best_params = {'max_depth': 17, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 190}\n# rf_best_params = {'max_depth': 3, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 190}\n# rf_best = RandomForestClassifier(class_weight=class_weight, random_state=RANDOM_SEED, **rf_best_params)\n# rf_best.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate_model(rf_best, plot_cmat=True, on = \"train\")\n# evaluate_model(rf_best, plot_cmat=True, on = \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Catboost","metadata":{}},{"cell_type":"markdown","source":"Next, we will consider the Catboost model.","metadata":{}},{"cell_type":"code","source":"cat_base = CatBoostClassifier(class_weights=class_weight, random_state=RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_base.fit(X_train, y_train, verbose = 0)\nevaluate_model(cat_base, plot_cmat=True, on = \"train\")\nevaluate_model(cat_base, plot_cmat=True, on = \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cat Tuning Round 1","metadata":{}},{"cell_type":"code","source":"# cat_param_grid = {\n#     \"iterations\": [50, 100, 150],\n#     \"depth\": [5, 8,  10],\n#     \"learning_rate\": [0.01, 0.05, 0.1],\n#     \"l2_leaf_reg\": [1, 3, 5],\n# }\n\n# loss_function = \"Logloss\"\n\n# cat_grid = grid_search(cat_param_grid, CatBoostClassifier(loss_function=loss_function, class_weights=class_weight, random_state=RANDOM_SEED, verbose = 0), verbose = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cat Tuning Round 2","metadata":{}},{"cell_type":"code","source":"# cat_param_grid = {\n#     \"iterations\": [40, 50, 70],\n#     \"depth\": [8, 10, 12],\n#     \"learning_rate\": [0.005, 0.01, 0.02],\n#     \"l2_leaf_reg\": [4, 5, 6],\n# }\n\n# loss_function = \"Logloss\"\n\n# cat_grid = grid_search(cat_param_grid, CatBoostClassifier(loss_function=loss_function, class_weights=class_weight, random_state=RANDOM_SEED, verbose = 0),\n#                        verbose = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cat Tuning Round 3","metadata":{}},{"cell_type":"code","source":"# cat_param_grid = {\n#     \"iterations\": [30, 40, 50],\n#     \"depth\": [12, 15],\n#     \"learning_rate\": [0.005],\n#     \"l2_leaf_reg\": [6],\n# }\n\n# loss_function = \"Logloss\"\n\n# cat_grid = grid_search(cat_param_grid, CatBoostClassifier(loss_function=loss_function, class_weights=class_wieght, random_state=RANDOM_SEED, verbose = 0),\n#                        verbose = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate_model(cat_best, plot_cmat=True, on = \"train\")\n# evaluate_model(cat_best, plot_cmat=True, on = \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat_param_grid = {\n#     \"iterations\": [40, 50, 70],\n#     \"depth\": [8, 10, 12],\n#     \"learning_rate\": [0.005, 0.01, 0.02],\n#     \"l2_leaf_reg\": [4, 5, 6],\n# }\n\n# loss_function = \"Logloss\"\n\n# cat_grid = grid_search(cat_param_grid, CatBoostClassifier(loss_function=loss_function, class_weights=class_wieght, random_state=RANDOM_SEED, verbose = 0),\n#                        verbose = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost.metrics import Logloss\n\nlog_loss = Logloss(use_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    param = {\n        \"iterations\": trial.suggest_int(\"iterations\", 30, 100),\n        \"depth\": trial.suggest_int(\"depth\", 5, 15),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05),\n        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 10),\n    }\n    cat = CatBoostClassifier(loss_function=log_loss, class_weights=class_weight, random_state=RANDOM_SEED,\n                             verbose = 0, **param)\n    cat.fit(X_train, y_train)\n    return balanced_log_loss(y_test, cat.predict_proba(X_test))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}